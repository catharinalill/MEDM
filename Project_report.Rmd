---
title: "Project MEDM"
output: html_document
        #pdf_document
---

<!-- Press Knit, to make a html-file of the code below. This we can use while working with the report to see how it is. Because the dataset flights in pretty large it takes some time to knit the file.. 
We don´t have to compilate the code to a pdf before we are delivering it, but if you want to you might have to download a program, maybe Latex or something, I am not quite sure. I already have it so I can compilate it to a pdf when we are done with it. 
But remember if you compilate it to a pdf-file some lines can be too long and get cutted. Then you have to go in the code and adjust it. If you need a linespace you need to have 3 space in the line above. -->

<!-- In gitkranken: 
When you have edited the code, save it usual. Then open gitkranken, and you will see a blue line with 
" x file changes in working directory" and a small box "View changes" in the above right corner. Click this one. First you have to commit what you have done, i.e. click on the Project_report.html-file on the right side and press Stage File. (And don´t stage all the other files) Then make a comment on what you have changed in the file, in the bottom right corner, and press the green box "Stage files/changes to commit". 
After commiting you first have to pull what the other have done, i.e. press "Pull". If you get "pulled successfully" down in the left corner everything is okay and you can press "Push". 
If not, then we might have some merge conflict we need to fix. 
Always remember to do all these steps
1. Commit
2. Pull
3. Push

If you have edited the file, but don´t want to save it, only download what the others have done, you can press the "Stash" instead of pull and push. 
-->

```{r setup, include=FALSE}
#Have to include the files in the same folder the Rmarkdown is in.
airlines = read.csv("airlines.csv")
airports = read.csv("airports.csv")
flights = read.csv("flights.csv")

```

   
```{r echo = FALSE}  
 #to prevent the code chunk from printing of the R code

```

<!-- This is a comment. -->

<!--
Here we can write Latex code, with formulas inline $E=mc^2$, or in a new line,
$$
    A = \pi \cdot r^2.
$$
This is standard Latex-code, so we can write all mathematical signs by using $\mathbf{A}x = b$. 
$\hat{Y}$, $Y_i$, $\theta$, $\rho(X,C)$, $x_1^{(1)}$. 
\textbf{bold text}, \textit{italicized}
This link includes a list of mathematical symbols written in latex. https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols
-->

   <!-- 1 # means Section  -->
# PART 1: EXPLORATORTY DATA ANALYSIS 

 <!-- Perform a exploratory data analysis and discuss what you have learned from this analysis.
 
 
 Aim: Which airline should we fly on? 
 
The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics tracks the on-time performance of domestic flights operated by large air carriers. 
Summary information on the number of on-time, delayed, canceled, and diverted flights is published in DOT's monthly Air Travel Consumer Report and in this dataset of 2015 flight delays and cancellations.

Initial steps: 
1-	To solve the classification problem the group must create a new variable through the analysis of the variables associated to the delay that can be used as response variable for the problem aim. 
 
 -->


```{r , echo=TRUE}
library(ggplot2)  #for ggplot
library(class)    #for confusion matrices
library(caret)    #for knn function
library(e1071)     #for naive bayes function
library(GGally)
library(corrplot)
library(DataExplorer)
library(dplyr)
library(MASS)     #for LDA
library(leaps)    #for Forward selection

#NEED TO CREATE RESPONSE VARIABLE HERE


#The first variable in our flights dataset is YEAR = 2015, which is equal for all so we should not include
# this in the data analysis/classification.

#DATA ANALYSIS

#plot_str(flights[,c(2:10)]) #When we have found the response variable add this: c(2:10,response variable)

#Search for missing values
plot_missing(flights)

#Continuous variables
#plot_histogram(flights[,2:10])
#plot_density(flights[,2:10])

#Correlation
plot_correlation((flights[, 2:31]), type = "continuous")
#or
#corrplot(cor(flights),method = "circle")

#plot_bar(flights)

#Check the response variable, plot a histogram and see the distribution, could us qqplot
#ggplot(data = flights)

#library(DataExplorer)
#create_report(flights)
#Check for outliers
#boxplot(flights[,1]~flights[,101], col=c(1:2))
```


```{r , echo=TRUE}
#Create another data variable

##Remove the observations for which the flight was cancelled                                            (THIS CHOICE SHOULD BE JUSTIFIED IN THE REPORT!)
handled_flights <- flights[flights$CANCELLED<1,]  
#Remove columns YEAR (2015 for all observations), CANCELLATION_REASON (since we are now dealing with flights that 
#were not cancelled), CANCELLED (will have 0's since these are the flights that were not cancelled)
#and DIVERTED (will have 1's for observations where ARRIVAL DELAY = NA,so we may remove this column)
handled_flights$YEAR <- NULL
handled_flights$CANCELLATION_REASON <- NULL
handled_flights$CANCELLED <- NULL
handled_flights$DIVERTED <- NULL

#Erase columns which have NA for some observations (WE CAN CONSIDER SETTING THIS NA CELLS TO ZERO)
handled_flights$AIR_SYSTEM_DELAY <- NULL
handled_flights$SECURITY_DELAY <- NULL
handled_flights$AIRLINE_DELAY <- NULL
handled_flights$LATE_AIRCRAFT_DELAY <- NULL
handled_flights$WEATHER_DELAY <- NULL

#Erase the observations for which there was no information concerning the arrival delay
handled_flights<- handled_flights[is.na(handled_flights$ARRIVAL_DELAY)==FALSE,]

#Create list of existent airports
airports_list <- airports$IATA_CODE

#Create list of existent airlines
airlines_list <- airlines$IATA_CODE

#Erase observations for origin_airports, destination_airports which are not contained in the data frame airports and
#airlines which are not contained  in the data frame airlines
handled_flights<- handled_flights[handled_flights$ORIGIN_AIRPORT %in% airports_list,]
handled_flights<- handled_flights[handled_flights$DESTINATION_AIRPORT %in% airports_list,]
handled_flights<- handled_flights[handled_flights$AIRLINE %in% airlines_list,]

#Find the percentages of each airline
n = dim(handled_flights)[1]
counts <- table(handled_flights$AIRLINE)
percentage <- counts/n
percentage

#Sample 100.000 of original data set 
flights_small <- sample_n(handled_flights, 100000)                                                      #(THIS CHOICE SHOULD BE JUSTIFIED IN THE REPORT!)
#Find the percentages of each airline in small dataset
n2 = dim(flights_small)[1]
counts2 <- table(flights_small$AIRLINE)
percentage2 <- counts2/n2
percentage2
#Comment that we see that these percentages is the same so this is okay to do

#Count by airline who many flights are delayed in handled_flights
handled_flightsdelayed<- handled_flights[handled_flights$ARRIVAL_DELAY>0,]
counts3 <- table(handled_flightsdelayed$AIRLINE)
percentdelayedbyairline <- counts3/counts

#Count by airline who many flights are delayed in flights_small
flights_smalldelayed<- flights_small[flights_small$ARRIVAL_DELAY>0,]
counts4 <- table(flights_smalldelayed$AIRLINE)
percentdelayedbyairline2 <- counts4/counts2

#Check if the variable distributions are mantained after sampling - YES!
create_report(flights_small, output_file = 'report_flights_small.html')
create_report(handled_flights, output_file = 'report_handled_flights.html')


#To decide how many minutes define small and big delays, we plot an histogram and consider the median/mean value, for example, considering only the values
#of ARRIVAL_DELAY > 0, which represent delays
median(handled_flights$ARRIVAL_DELAY[handled_flights$ARRIVAL_DELAY>0]) #mean = 33.11 median = 15  

#I guess we should consider: no delay: <= 0 min, small delay: 1-15 min, medium delay: 15-33 min, big delay: >33 min  (THIS CHOICE SHOULD BE JUSTIFIED IN THE REPORT!)
flights_small$DELAY = ifelse(flights_small$ARRIVAL_DELAY < 1,flights_small$DELAY <- 0,
              ifelse(flights_small$ARRIVAL_DELAY <16,flights_small$DELAY <- 1, 
                     ifelse(flights_small$ARRIVAL_DELAY <34,flights_small$DELAY <- 2, flights_small$DELAY <- 3)))


#Since our objetive is to predict the delay of a flight, we might walk a mile in someone else's shoes, this time, in the shoes of someone who is about to take a flight.
#So, we must give to the classifier, only the variables correspondent to subjects known a priori: scheduled departure, distance, etc.

#Maintain a priori columns and remove a posteriori columns
flights_to_predict <- flights_small[,]
flights_to_predict$DEPARTURE_TIME <- NULL
flights_to_predict$DEPARTURE_DELAY <- NULL
flights_to_predict$TAXI_OUT <- NULL                                                            #(THIS CHOICE SHOULD BE JUSTIFIED IN THE REPORT!)
flights_to_predict$WHEELS_OFF <- NULL
flights_to_predict$ELAPSED_TIME <- NULL
flights_to_predict$AIR_TIME <- NULL
flights_to_predict$DISTANCE <- NULL
flights_to_predict$WHEELS_ON <- NULL
flights_to_predict$TAXI_IN <- NULL
flights_to_predict$ARRIVAL_TIME <- NULL
flights_to_predict$ARRIVAL_DELAY <- NULL


```




```{r , echo=TRUE}

#Correlation
plot_correlation((flights_small[, 1:22]), type = "continuous")

#Looking at the correlation plot we remove variable with correlation coefficient larger than 0.9
handled_flights_small = flights_small[,]
handled_flights_small$DEPARTURE_DELAY <- NULL
handled_flights_small$WHEELS_ON <- NULL
handled_flights_small$WHEELS_OFF <- NULL
handled_flights_small$SCHEDULED_ARRIVAL <- NULL
handled_flights_small$SCHEDULED_DEPARTURE <- NULL
handled_flights_small$ELAPSED_TIME <- NULL
handled_flights_small$AIR_TIME <- NULL
handled_flights_small$DISTANCE <- NULL

plot_correlation((handled_flights_small[, 1:14]), type = "continuous")

#Categorical variables
plot_bar(handled_flights_small)

#SHOULD WE CONSIDER THE CATEGORICAL VARIABLES HERE??

################### Another approach of variable selection #########################
#Try Forward Selection
regfit.full = regsubsets(DELAY~.,data = flights_small,method = "forward")
reg.summary = summary(regfit.full)

#Plot adjusted R^2, C_p and BIC for all the models at once
par(mfrow =c(3,2))

plot(reg.summary$adjr2,xlab="Number of Variables", ylab = "Adjusted RSq", type = "1")
adjr2_max = which.max(reg.summary$adjr2)
points(adjr2_max, reg.summary$adjr2[adjr2_max], col = "red", cex = 2, pch = 20)

plot(reg.summary$bic,xlab="Number of Variables", ylab = "BIC", type = "1")
bic_min = which.min(reg.summary$bic)
points(bic_min, reg.summary$bic[bic_min], col = "red", cex = 2, pch = 20)

plot(reg.summary$cp,xlab="Number of Variables", ylab = "Cp", type = "1")
cp_min = which.min(reg.summary$cp)
points(cp_min, reg.summary$cp[cp_min], col = "red", cex = 2, pch = 20)

#From this we can choose the "best" model and use this subset of variables to do the classification



```



# PART 2: SOLVE CLASSIFICATION PROBLEM

 <!-- Solve your classification problem.  Consider several classification methods and discuss  how  can  they  contribute  for  the  solution  of  your  problem.   Include in your discussion topics such as the options that you have made in building each  classifier;  interpretation  of  results;  validation  of  the  methods  used  and possible assumptions;  advantages and disadvantages of each alternative;  etc. Have  in  mind  that  some  of  the  explanatory  variables  may  be  irrelevant  to the  classification problem and that  you may need to  do  some  preprocessing methodologies of your data set e.g.  dimensionality reduction techniques.-->

```{r , echo=TRUE}
#We will use Hold Out Method to partition the sample in train, validation and test data

#Split sample in train (0.7%), validation (0.15) and test (0.15)
#Train: create the model
#Validation: choose parameters for classifiers
#Test: evaluate the fit of the classifier and compare it with the others

set.seed(0)
num_obs <- dim(flights_to_predict)[1]
index_list <- c(1:num_obs)
train_index <- sample(index_list, num_obs*0.7)
test_valid_index <- setdiff(index_list, train_index)
test_index <- sample(test_valid_index, length(test_valid_index)*0.5)
valid_index <- setdiff(test_valid_index, test_index)

train_data <- flights_to_predict[train_index,]
train_data$DELAY <- NULL

train_data_y <- (flights_to_predict[train_index,])$DELAY

valid_data <- flights_to_predict[valid_index,]
valid_data$DELAY <- NULL

valid_data_y <- (flights_to_predict[valid_index,])$DELAY

test_data <- flights_to_predict[test_index,]
test_data$DELAY <- NULL

test_data_y <- (flights_to_predict[test_index,])$DELAY



```
 <!-- 2 ## means subsection, 3 means subsubsection etc.-->   

## Naive Bayes method

```{r , echo=TRUE}

modelNB <- naiveBayes(DELAY ~ ., data = flights_to_predict[train_index,])
predictNB <- predict(modelNB, test_data)
table(predictNB, test_data_y)
#Not working - define data types as they should be



```

## Resubstitution

```{r , echo=TRUE}


```


## Hold Out method

```{r , echo=TRUE}
set.seed(0) # To fix the random process at splitting data
# Spliting data as training and test set using createDataPartition() function from caret
indxTrain <- createDataPartition(y = handled_flights_small[,15], p = 0.8,list = FALSE)
train.data <- handled_flights_small[indxTrain,]
test.data <- handled_flights_small[-indxTrain,]

#Checking distribution in original and partitioned data
prop.table(table(train.data$y)) * 100 # at training 
prop.table(table(test.data$y)) * 100  # at testing
prop.table(table(handled_flights_small$y)) * 100  # at original dataset
#WHY DOES THIS NOT WORK??

#Make model
pred.knn.m1 <- kknn(formula = formula(train.data[,15]~.), train = train.data, test = test.data, k = 5, distance = 1)
fit.m1<-fitted(pred.knn.m1)

#Classification error
mean(test.data[,15] != fit.m1)

#Accuracy
mean(test.data[,15] == fit.m1)

#Confusion matrix
table(test.data[,15] , fit.m1)

#if predicted value is 0 (i.e. not delayed), find the airline and make a count
#Find which airlines that are delayed and not
not_delayed = test.data[test.data$y == 0,]
some_delay = test.data[test.data$y == 1,]
delayed = test.data[test.data$y == 2,]

count_0 <- table(not_delayed$AIRLINES)
count_1 <- table(some_delayed$AIRLINES)
count_2 <- table(delayed$AIRLINES)


```
The training and test set will be dependent on which observations are included in each of them and this could lead to high variability of validation set error. Another disadvantage with hold out method is that due to dividing the training set in two, the sample size for model fitting will be smaller. 



## LOOCV

```{r , echo=TRUE}


```
The advantages with LOOCV is that there is no randomsness in splitting the data into training and test set. There is also little bias since nearly the whole data set is used for training, compared to only half the data set for Hold Out method Further, one disadvantage with LOOCV is the expensive implementation because we need to fit $n$ different models. Due to quite similar training sets, since they only differ by one observation, each fold will be highly correlated and this could give high variance for the LOOCV approach as well. 
   
## K-fold CV

```{r , echo=TRUE}
# Spliting data as training and test set using createDataPartition() function from caret
indxTrain <- createDataPartition(y = handled_flights[,23], p = 0.8,list = FALSE)
train.data <- handled_flights[indxTrain,]
test.data <- handled_flights[-indxTrain,]

train.control <- trainControl(method = "cv",number = 10) 
# Train the model
set.seed(0) # To fix the random process at splitting data
pred.knn.cv <- train(CL~.,
                     method     = "knn",
                     tuneGrid   = expand.grid(k = 1:20),
                     trControl  = train.control,
                     metric     = "Accuracy",
                     data       = cbind(CL = as.factor(train.data[,23]), train.data[,-23]))
#Error: vector memory exhausted (limit reached?)


# Summarize the results
print(pred.knn.cv)
plot(pred.knn.cv)

# Prediction of fitted model
fit_cv <- predict(pred.knn.cv,newdata = test.data)

# Make confusion matrix 
confusionMatrix(test.data$y, fit_cv)

# Calculate accuracy
mean(test.data$y == fit_cv)

# Calculate misclassification error 
mean(test.data$y != fit_cv)


```

One disadvantage with using K-fold cross validation is that the result may vary according to how the folds are made. Since the training set is $(k-1)/k$ of the original data set, this will give an estimate of the prediction error that is biased upwards. 

One advantage with cross validation over the LOOCV approach is that there is less computational work when we have $k=10$, instead of $K=N$. 
By considering the Bias-Variance trade-off we have smaller variance for the CV approach than the LOOCV, but again LOOCV has smaller bias. 


## Repeated K-fold CV

```{r , echo=TRUE}
train.control <- trainControl(method = "repeatedcv", 
                              number = 5, repeats = 4)
set.seed(0)
pred.knn.rcv <- train(CL~.,
                     method     = "knn",
                     tuneGrid   = expand.grid(k = 1:20),
                     trControl  = train.control,
                     metric     = "Accuracy",
                     data       = cbind(CL = as.factor(train.data[,23]), train.data[,-23]))
# Summarize the results
print(pred.knn.rcv)
plot(pred.knn.rcv)

# Prediction of fitted model
fit_rcv <- predict(pred.knn.rcv,newdata = test.data)

# Make confusion matrix 
confusionMatrix(test.data$y, fit_rcv)

# Calculate accuracy
mean(test.data$y == fit_rcv)

# Calculate misclassification error 
mean(test.data$y != fit_rcv)

```



## Bootstrap

```{r , echo=TRUE}


```



# PART 3: LEARNING/LIMITATIONS/FUTURE WORK

 <!-- Imagine you are going to meet the researcher who contacted you.  Report to him/her what you have learned about the problem.  Discuss limitations of the analysis you have done and provide suggestions for future work.-->


```{r , echo=TRUE}


```

